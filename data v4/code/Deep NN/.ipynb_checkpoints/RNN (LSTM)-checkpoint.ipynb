{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent neural network\n",
    "- A recurrent neural network (RNN) is a class of artificial neural networks where connections between nodes form a directed graph along a temporal sequence. This allows it to exhibit temporal dynamic behavior. Derived from feedforward neural networks, RNNs can use their internal state (memory) to process variable-length sequences of inputs. This makes them applicable to tasks such as unsegmented, connected handwriting recognition, or speech recognition.\n",
    "\n",
    "### Long short-term memory\n",
    "- Long short-term memory (LSTM) is an artificial recurrent neural network (RNN) architecture used in the field of deep learning. Unlike standard feedforward neural networks, LSTM has feedback connections. It can not only process single data points but also entire sequences of data.\n",
    "    - For example, LSTM is applicable to tasks such as handwriting recognition, speech recognition and anomaly detection in network traffic.\n",
    "    - A common LSTM unit is composed of a cell, an input gate, an output gate and a forget gate. The cell remembers values over arbitrary time intervals and the three gates regulate the flow of information into and out of the cell.\n",
    "    - LSTMs were developed to deal with the vanishing gradient problem that can be encountered when training traditional RNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11077, 27)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import library\n",
    "import numpy as np # linear algebra\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Masking, Embedding\n",
    "\n",
    "import statistics \n",
    "from statistics import mode \n",
    "\n",
    "# load data\n",
    "data = pd.read_csv(\"../../data.csv\")\n",
    "# Get valuse from data (datafram)\n",
    "data = data.values\n",
    "X = data[:,0:6] # all rows, no Lebal\n",
    "y = data[:,6] # all rows, label only\n",
    "\n",
    "# Define PolynomialFeatures variable with degree 5 and without bias\n",
    "poly = PolynomialFeatures(degree=2 , include_bias=False)\n",
    "X=poly.fit_transform(X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error in i =  259\n",
      "error in i =  463\n",
      "error in i =  641\n",
      "error in i =  794\n",
      "error in i =  1207\n",
      "error in i =  1411\n",
      "error in i =  1590\n",
      "error in i =  1743\n",
      "error in i =  1936\n",
      "error in i =  2267\n",
      "error in i =  2445\n",
      "error in i =  3011\n",
      "error in i =  3685\n",
      "error in i =  4887\n",
      "error in i =  5040\n",
      "error in i =  5244\n",
      "error in i =  5758\n",
      "error in i =  6140\n",
      "error in i =  6578\n",
      "error in i =  6705\n",
      "error in i =  7226\n",
      "error in i =  7333\n",
      "error in i =  7796\n",
      "error in i =  7878\n",
      "error in i =  8239\n",
      "error in i =  8479\n",
      "error in i =  8682\n",
      "error in i =  8789\n",
      "error in i =  8992\n",
      "error in i =  9782\n",
      "error in i =  10781\n",
      "(10986,)\n",
      "(10986, 60, 27)\n"
     ]
    }
   ],
   "source": [
    "features_set = []\n",
    "lebals = []\n",
    "\n",
    "for i in range(60,11077):\n",
    "    try:\n",
    "        List = list(y[i-60:i])\n",
    "        val = mode(List)\n",
    "\n",
    "        xx = X[i-60:i,:]\n",
    "        xx = np.array(xx)\n",
    "        \n",
    "        features_set.append(xx)\n",
    "        lebals.append(val)\n",
    "    except:\n",
    "         print(\"error in i = \",i)\n",
    "\n",
    "lebals = np.array(lebals)\n",
    "features_set = np.array(features_set)\n",
    "\n",
    "print(lebals.shape)\n",
    "print(features_set.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- split dataset to train and test dataset\n",
    "    - with ratio 75% to train the dataset and 20% to the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train sahse =  (8788, 60, 27)\n",
      "X_test sahse =  (2198, 60, 27)\n",
      "y_train sahse =  (8788,)\n",
      "y_test sahse =  (2198,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features_set, lebals, test_size=0.20, random_state=42)\n",
    "print('X_train sahse = ',X_train.shape)\n",
    "print('X_test sahse = ',X_test.shape)\n",
    "print('y_train sahse = ',y_train.shape)\n",
    "print('y_test sahse = ',y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN (LSTM) Model\n",
    "- create the LSTM model\n",
    "- compile the LSTM model\n",
    "- print the LSTM model summary\n",
    "- train(Fit) the LSTM model\n",
    "- Predict the output of X_test using LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have trained a model using Simple RNN and it got accuracy 70% and then it fell in vanishing gradient problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " ##############################################\n",
    "# model.add(layers.SimpleRNN(units= 32 , input_shape=(60,27),activation='relu'))\n",
    "# model.add(tf.keras.layers.Dense(128, activation=tf.nn.sigmoid))\n",
    "# model.add(tf.keras.layers.Dense(64, activation=tf.nn.sigmoid))\n",
    "# model.add(tf.keras.layers.Dense(16, activation=tf.nn.sigmoid))\n",
    "# model.add(Dense(7, activation='softmax'))\n",
    "# model.compile(\n",
    "#     loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "#     optimizer=\"adam\",\n",
    "#     metrics=[\"accuracy\"],\n",
    "# )\n",
    "#########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### create the LSTM model\n",
    "- LSTM layer\n",
    "    - the dimensionality of the output space (units)\n",
    "    - return_sequences (Whether to return the last output in the output sequence or the full sequence).\n",
    "- Dropout\n",
    "    - The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting.\n",
    "\n",
    "- Layers\n",
    "    - LSTM with (unit = 50 ,return_sequences=True,input_shape =(60,27) )\n",
    "    - Dropout with rate = 0.2\n",
    "    \n",
    "    - LSTM with (unit = 50 ,return_sequences=True)\n",
    "    - Dropout with rate = 0.2\n",
    "    \n",
    "    - LSTM with (unit = 50 ,return_sequences=True)\n",
    "    - Dropout with rate = 0.2\n",
    "    \n",
    "    - LSTM with (unit = 50)\n",
    "    - Dropout with rate = 0.2\n",
    "    \n",
    "    - fully-connected layer (dense) with 128 nodes and sigmoid activation function\n",
    "    - fully-connected layer (dense) with 7 nodes and softmax activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(layers.LSTM(units = 50,\n",
    "                     return_sequences=True ,\n",
    "                     input_shape =(60,27)))\n",
    "model.add(layers.Dropout(0.2))\n",
    "\n",
    "model.add(layers.LSTM(units = 50,\n",
    "                     return_sequences=True))\n",
    "model.add(layers.Dropout(0.2))\n",
    "\n",
    "model.add(layers.LSTM(units = 50,\n",
    "                     return_sequences=True))\n",
    "model.add(layers.Dropout(0.2))\n",
    "\n",
    "model.add(layers.LSTM(units = 50))\n",
    "model.add(layers.Dropout(0.2))\n",
    "\n",
    "model.add(Dense(128, activation='sigmoid'))\n",
    "model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### compile the LSTM model\n",
    "- optimizer = adam\n",
    "- sparse_categorical_crossentropy loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### print the LSTM model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 60, 50)            15600     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 60, 50)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 60, 50)            20200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 60, 50)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 60, 50)            20200     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 60, 50)            0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 50)                20200     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               6528      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7)                 903       \n",
      "=================================================================\n",
      "Total params: 83,631\n",
      "Trainable params: 83,631\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### train(Fit) the LSTM model (epoch = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "275/275 [==============================] - 25s 91ms/step - loss: 1.2016 - accuracy: 0.4975 - val_loss: 0.8265 - val_accuracy: 0.6561\n",
      "Epoch 2/15\n",
      "162/275 [================>.............] - ETA: 10s - loss: 0.6305 - accuracy: 0.7488"
     ]
    }
   ],
   "source": [
    "hist = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predict the output of X_test using LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict=model.predict_classes(X_test)\n",
    "y_predict[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Model \n",
    "- calculate the evaluate model accuracy and loss (accuracy = 0.9959 , loss = 0.0143 ) \n",
    "- plot the training loss matrix\n",
    "- plot the training accuracy matrix\n",
    "- clasification report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the evaluate model accuracy and loss \n",
    "# (accuracy = 0.9959 , loss = 0.0143 )\n",
    "val_loss, val_acc = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training loss matrix\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(hist.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training accuracy matrix\n",
    "plt.plot(hist.history['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clasification report\n",
    "print(classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save the model after training.\n",
    "- so we can load it and run without the need to train anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.models.save_model(model,'RNN.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load the model and predict the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load_model = keras.models.load_model('saved_model')\n",
    "load_model = keras.models.load_model('NNM.h5')\n",
    "load_y_predict=load_model.predict_classes(X_test)\n",
    "load_y_predict[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluate the loaded model with classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, load_y_predict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
